{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12e9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from textwrap import dedent\n",
    "from agno.tools import Toolkit\n",
    "from pathlib import Path\n",
    "from agno.agent import Agent\n",
    "from agno.models.ollama import Ollama\n",
    "import difflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200aed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "path = \"OpenGVLab/InternVL2_5-1B\"\n",
    "vlm_model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_8bit=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5e17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e31813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from agno.agent import Agent\n",
    "from agno.models.ollama import Ollama\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "from agno.utils.media import download_image\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "from duckduckgo_search import DDGS\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "\n",
    "class ImageTool(Toolkit):\n",
    "    # @validate_call\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(name=\"image_tools\", **kwargs)\n",
    "        self.register(self.image_overview)\n",
    "        self.register(self.describe_image)\n",
    "        self.register(self.check_leaf_disease)\n",
    "        self.register(self.search_solutions)\n",
    "    def image_overview(self) -> str:\n",
    "        question = (\n",
    "            \"<image>\\n\"\n",
    "            \"Determine whether this image contains a tree leaf or any kind of plant leaf\"\n",
    "        )\n",
    "        pixel_values = load_image('./la1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "        response = vlm_model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "        return response\n",
    "    \n",
    "    def describe_image(self) -> str:\n",
    "        question = (\n",
    "            \"<image>\\n\"\n",
    "            \"Describe the content of this image in detail.\"\n",
    "        )\n",
    "        pixel_values = load_image('./la1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "        response = vlm_model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "        return response\n",
    "    \n",
    "    \n",
    "    def check_leaf_disease(self) -> str:  \n",
    "        pixel_values1 = load_image('./la1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "        pixel_values2 = load_image('./la2.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "        pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "        num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "        question = 'The original leaf image is: <image>\\n The leaf image with disease segmentation is: <image>\\nDescribe the leaf disease base on the original image and the leaf disease detect image.'\n",
    "        response = vlm_model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list)\n",
    "        return response\n",
    "    \n",
    "    def search_solutions(self, query: str, max_results: int = 5) -> str:\n",
    "        actual_max_results = max_results\n",
    "        search_query = query\n",
    "        ddgs = DDGS(\n",
    "        )\n",
    "        return json.dumps(ddgs.text(keywords=search_query, max_results=actual_max_results), indent=2)\n",
    "\n",
    "agent = Agent(\n",
    "    model=Ollama(id=\"qwen3:4b\"),\n",
    "    tools=[ImageTool()],\n",
    "    instructions = dedent(\"\"\"\\\n",
    "    You are an image processing assistant specialized in analyzing visual content provided by a tool. The image is already defined in the tool.\n",
    "\n",
    "    1. First, analyze the image to determine whether it is related to a leaf or a leaf disease.\n",
    "    2. If the image is **not** related to a leaf or leaf disease, return a clear message stating that, and provide a detailed overview of the image content.\n",
    "    3. If the image **is** related to a leaf, check for signs of disease.\n",
    "    4. If the leaf shows signs of disease, search for solutions to suggest possible causes or treatments with the query is signs of diseases.\n",
    "\"\"\"),\n",
    "    markdown=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b352e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant image: /home/veronrd/chatbot/VLMs/images/input/00007.jpg with similarity: 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Prepare model - ResNet50 without final fc layer\n",
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # Use all layers except final fc\n",
    "        self.features = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        return x\n",
    "\n",
    "# Load pretrained ResNet50\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "model = FeatureExtractor(resnet)\n",
    "model.eval()\n",
    "\n",
    "# 2. Prepare image transform (resize, normalize as ImageNet)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "    )\n",
    "])\n",
    "\n",
    "def extract_feature(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_t = transform(img)\n",
    "    batch_t = img_t.unsqueeze(0)  # batch dimension\n",
    "    with torch.no_grad():\n",
    "        feat = model(batch_t)\n",
    "    return feat\n",
    "\n",
    "# 3. Extract features for all images in folder\n",
    "folder_path = '/home/veronrd/chatbot/VLMs/images/input'\n",
    "folder_images = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "                 if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "folder_features = []\n",
    "for img_path in folder_images:\n",
    "    feat = extract_feature(img_path)\n",
    "    folder_features.append((img_path, feat))\n",
    "\n",
    "# 4. Extract feature for input image\n",
    "input_image_path = '00007.jpg'\n",
    "input_feat = extract_feature(input_image_path)\n",
    "\n",
    "# 5. Compute similarity with each folder image feature (cosine similarity)\n",
    "def cosine_sim(a, b):\n",
    "    return F.cosine_similarity(a, b).item()\n",
    "\n",
    "best_match = None\n",
    "best_score = -1  # cosine similarity ranges from -1 to 1\n",
    "for path, feat in folder_features:\n",
    "    sim = cosine_sim(input_feat, feat)\n",
    "    if sim > best_score:\n",
    "        best_score = sim\n",
    "        best_match = path\n",
    "\n",
    "print(f\"Most relevant image: {best_match} with similarity: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df550968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's start by looking at the user's image description. The image is a gradient background from pink to yellow with no objects or text. The user mentioned it's not related to a leaf or leaf disease, so I need to confirm that.\n",
      "\n",
      "First, I'll use the image_overview function to get a detailed look. But wait, the user already provided an overview. They said there's no leaf or disease. But maybe I should check if there's any possible misinterpretation. The description says no distinct objects, so probably not a leaf. \n",
      "\n",
      "Next, since the image isn't related to a leaf, I should inform the user. The task is to check if the image is related to a leaf or disease. Since it's a gradient background with nothing else, the answer is clear. The user might have thought there was a leaf, but the description says otherwise. So I'll explain that the image isn't related and describe it in detail as given.\n",
      "</think>\n",
      "\n",
      "The image is **not** related to a leaf or leaf disease. \n",
      "\n",
      "### Detailed Overview:\n",
      "- **Background**: A smooth gradient transitioning from **warm pink at the top** to **soft yellow at the bottom**.\n",
      "- **Content**: No distinct objects, people, text, patterns, or actions are present.\n",
      "- **Visual Effect**: A uniform, soothing background with no focal points or abnormalities.\n",
      "\n",
      "This image appears to be a purely abstract color gradient with no biological or agricultural elements.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\n",
    "   \n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
